{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: datasets\n",
      "Version: 1.4.1\n",
      "Summary: HuggingFace/Datasets is an open library of NLP datasets.\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /Users/chris/opt/anaconda3/lib/python3.8/site-packages\n",
      "Requires: fsspec, pandas, numpy, multiprocess, requests, huggingface-hub, dill, tqdm, pyarrow, xxhash\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"pos\"\n",
    "model_checkpoint = \"YituTech/conv-bert-base\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pos2021 (/Users/chris/.cache/huggingface/datasets/pos2021/Pos2021/1.0.0/0482b17250e196910190b085dead5d88af494b4d637ff012008084d9b5499fa4)\n"
     ]
    }
   ],
   "source": [
    "#datasets2 = load_dataset(\"conll2003\")\n",
    "\n",
    "datasets = load_dataset('datasetloader2.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'pos_tags'],\n",
       "        num_rows: 310\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'pos_tags'],\n",
       "        num_rows: 27\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'pos_tags'],\n",
       "        num_rows: 310\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'pos_tags'],\n",
       "        num_rows: 27\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos_tags': [23,\n",
       "  24,\n",
       "  11,\n",
       "  45,\n",
       "  43,\n",
       "  12,\n",
       "  23,\n",
       "  23,\n",
       "  16,\n",
       "  12,\n",
       "  23,\n",
       "  26,\n",
       "  16,\n",
       "  24,\n",
       "  38,\n",
       "  40,\n",
       "  17,\n",
       "  26,\n",
       "  6,\n",
       "  10,\n",
       "  12,\n",
       "  23,\n",
       "  26,\n",
       "  16,\n",
       "  26,\n",
       "  10,\n",
       "  26,\n",
       "  42,\n",
       "  43,\n",
       "  43,\n",
       "  45,\n",
       "  43,\n",
       "  11,\n",
       "  7],\n",
       " 'tokens': ['Typhoon',\n",
       "  'No.',\n",
       "  '17',\n",
       "  'has',\n",
       "  'caused',\n",
       "  'the',\n",
       "  'education',\n",
       "  'system',\n",
       "  'of',\n",
       "  'the',\n",
       "  'disaster',\n",
       "  'areas',\n",
       "  'of',\n",
       "  'Zhejiang',\n",
       "  'to',\n",
       "  'suffer',\n",
       "  'heavy',\n",
       "  'losses',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'number',\n",
       "  'schools',\n",
       "  'with',\n",
       "  'dormitories',\n",
       "  'and',\n",
       "  'facilities',\n",
       "  'having',\n",
       "  'been',\n",
       "  'damaged',\n",
       "  'has',\n",
       "  'reached',\n",
       "  '1500',\n",
       "  '.']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=50, names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', '-LRB-', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', '-RRB-', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], names_file=None, id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].features[f\"pos_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " \"''\",\n",
       " '#',\n",
       " '$',\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " '``',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'HYPH',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'LS',\n",
       " '-LRB-',\n",
       " 'MD',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'NN|SYM',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " '-RRB-',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'SYM',\n",
       " 'TO',\n",
       " 'UH',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'WRB']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PRP, VBD, ,, UH, RB, PRP, RB, MD, RB, VB, IN, PRP, .]</td>\n",
       "      <td>[I, said, ,, well, then, I, still, can, not, help, with, it, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[NNP, NNP, VBD, IN, DT, NN, TO, VB, DT, NN, ,, CC, PRP, RB, RB, VBD, IN, IN, IN, PRP, PRP, MD, RB, VB, RP, IN, NN, .]</td>\n",
       "      <td>[Zheng, Bing, thought, of, a, way, to, resolve, this, problem, ,, but, she, absolutely, never, imagined, that, because, of, it, she, would, nearly, end, up, in, prison, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[DT, NN, VBD, DT, NN, ,]</td>\n",
       "      <td>[The, bank, investigated, the, matter, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[PRP, MD, RB, VB, PRP, DT, NN, ,]</td>\n",
       "      <td>[You, ca, n't, do, it, this, way, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[DT, NNS, VBP, VBN, IN, DT, NN, VBZ, VBN, IN, DT, NNS, POS, NNS, IN, JJ, NNS, .]</td>\n",
       "      <td>[The, peasants, have, seen, that, the, association, has, safeguarded, for, the, peasants, ', interests, at, crucial, times, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[IN, NN, VBD, TO, VB, PRP, RP, ,, PRP, MD, RB, VB, PRP, RP, .]</td>\n",
       "      <td>[If, nobody, wanted, to, pay, it, back, ,, I, would, gradually, pay, it, off, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[VBN, DT, JJ, NN, IN, NN, NN, ,, PRP, MD, VB, TO, VB, RB, CD, NN, DT, NN, .]</td>\n",
       "      <td>[Given, the, seasonal, nature, of, peasant, income, ,, they, 'd, have, to, invest, about, 50, yuan, each, time, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[PRP, VBP, RB, VBN, NNP, NNS, IN, PRP, VBP, VBG, TO, VB, RP, DT, NNS, IN, DT, NNS, RB, IN, IN, DT, RB, JJ, NN, MD, VB, VBN, ,, RB, RB, IN, DT, JJ, NN, ,, CC, RB, IN, DT, NN, IN, DT, NNP, IN, NNP, .]</td>\n",
       "      <td>[They, have, indeed, threatened, Milosevic, allies, that, they, are, going, to, bring, out, the, people, onto, the, streets, again, so, that, a, completely, new, authority, can, be, established, ,, not, just, on, the, federal, level, ,, but, also, on, the, level, of, the, Republic, of, Serbia, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[RB, RB, RB, ,, NNP, NNP, CC, DT, JJ, NNS, IN, DT, NNS, POS, NN, VBD, NN, WDT, VBD, DT, NNS, TO, VB, DT, JJ, NNS, IN, DT, NNS, POS, NN, .]</td>\n",
       "      <td>[Not, long, after, ,, Zheng, Bing, and, the, key, members, of, the, peasants, ', association, did, something, that, allowed, the, villagers, to, see, the, direct, advantages, of, the, peasants, ', association, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[RB, ,, IN, VBG, DT, NN, NN, ,, PRP, RB, VBD, NN, IN, JJ, NNS, ,, WDT, VBD, PRP, TO, VB, CD, NN, IN, NN, MD, RB, VB, VBN, .]</td>\n",
       "      <td>[However, ,, in, running, the, fertilizer, store, ,, she, often, gave, credit, to, poor, peasants, ,, which, made, her, to, have, 110,000, yuan, of, credit, could, not, be, collected, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ConvBertModel, ConvBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ConvBertTokenizerFast.from_pretrained(\"YituTech/conv-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.ConvBertTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"], is_split_into_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['More', 'than', '20', 'people', 'from', 'Wenzhou', \"'s\", 'Education', 'Committee', 'Organization', ',', 'offices', 'and', 'cadres', 'separated', 'into', '7', 'groups', 'to', 'go', 'to', 'such', 'places', 'as', 'Yueqing', ',', 'Ouhai', ',', 'Dongtou', ',', 'Pingyang', ',', 'etc.', 'to', 'help', 'finish', 'restoring', 'the', 'school', 'and', 'the', 'school', 'opening', 'preparation', 'work', '.']\n"
     ]
    }
   ],
   "source": [
    "example = datasets[\"train\"][4]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'more', 'than', '20', 'people', 'from', 'wen', '##zhou', \"'\", 's', 'education', 'committee', 'organization', ',', 'offices', 'and', 'cad', '##res', 'separated', 'into', '7', 'groups', 'to', 'go', 'to', 'such', 'places', 'as', 'yue', '##qing', ',', 'ou', '##hai', ',', 'dong', '##tou', ',', 'ping', '##yang', ',', 'etc', '.', 'to', 'help', 'finish', 'restoring', 'the', 'school', 'and', 'the', 'school', 'opening', 'preparation', 'work', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 56)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 5, 6, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 24, 25, 26, 26, 27, 28, 28, 29, 30, 30, 31, 32, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 8418, 25311, 6692, 2739, 4034, 1010, 6865, 9791, 1010, 2244, 3416, 1010, 2011, 12060, 15030, 9048, 5063, 21882, 1998, 8418, 16211, 2078, 21882, 102], [101, 15393, 2053, 1012, 2459, 2038, 3303, 1996, 2495, 2291, 1997, 1996, 7071, 2752, 1997, 26805, 2000, 9015, 3082, 6409, 1010, 1998, 1996, 2193, 2816, 2007, 19568, 15660, 3111, 1998, 4128, 2383, 2042, 5591, 2038, 2584, 10347, 1012, 102], [101, 2044, 1996, 2231, 1998, 1996, 2111, 1999, 1996, 7071, 2181, 8851, 4061, 1996, 7071, 1999, 2019, 2035, 1011, 2041, 3947, 2000, 3828, 3209, 1010, 2085, 1010, 2087, 1997, 1996, 7071, 2181, 1005, 1055, 3078, 1998, 2690, 2816, 2031, 2441, 2004, 5156, 1012, 102], [101, 26805, 2874, 1005, 1055, 2495, 2533, 2038, 3728, 2741, 2041, 2551, 2780, 2000, 10329, 2175, 2000, 2107, 3182, 2004, 19181, 9791, 1010, 13843, 9791, 1010, 4385, 1012, 2000, 3622, 1996, 2495, 6418, 2147, 1999, 1996, 7071, 2181, 1012, 102], [101, 2062, 2084, 2322, 2111, 2013, 19181, 9791, 1005, 1055, 2495, 2837, 3029, 1010, 4822, 1998, 28353, 6072, 5459, 2046, 1021, 2967, 2000, 2175, 2000, 2107, 3182, 2004, 27163, 19784, 1010, 15068, 10932, 1010, 11947, 24826, 1010, 17852, 12198, 1010, 4385, 1012, 2000, 2393, 3926, 16487, 1996, 2082, 1998, 1996, 2082, 3098, 7547, 2147, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 24, 24, 24, 24, 24, 6, 24, 24, 6, 24, 23, 6, 16, 26, 24, 24, 24, 24, 10, 24, 24, 24, 24, -100], [-100, 23, 24, 24, 11, 45, 43, 12, 23, 23, 16, 12, 23, 26, 16, 24, 38, 40, 17, 26, 6, 10, 12, 23, 26, 16, 26, 26, 26, 10, 26, 42, 43, 43, 45, 43, 11, 7, -100], [-100, 16, 12, 23, 10, 12, 26, 16, 12, 23, 23, 33, 41, 12, 23, 16, 12, 33, 15, 17, 23, 38, 40, 30, 6, 33, 6, 19, 16, 12, 23, 23, 29, 29, 17, 10, 17, 26, 44, 43, 16, 17, 7, -100], [-100, 24, 24, 29, 29, 24, 24, 45, 33, 43, 36, 42, 26, 38, 33, 40, 16, 17, 26, 16, 24, 24, 6, 24, 24, 6, 14, 14, 38, 40, 12, 23, 23, 23, 16, 12, 23, 23, 7, -100], [-100, 18, 16, 11, 26, 16, 24, 24, 29, 29, 24, 24, 24, 6, 26, 10, 26, 26, 41, 16, 11, 26, 38, 40, 16, 17, 26, 16, 24, 24, 6, 24, 24, 6, 24, 24, 6, 24, 24, 6, 14, 14, 38, 40, 40, 42, 12, 23, 10, 12, 23, 23, 23, 23, 7, -100]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_and_align_labels at 0x7fce2b457dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e1fbedde8645cca6e66f7598456121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0c5e75b48a4b84be39f5ae60c5dbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d589ea162eb847e09b2f62def43ab74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=674.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec25b4822b04eb9b13a10c3ac1eb44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=422840281.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConvBertForTokenClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ConvBertForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = ConvBertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: POS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: , seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: TO seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: FW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: . seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'BD': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'BG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'C': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'D': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'J': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'JR': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'N': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 7},\n",
       " 'NP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 6},\n",
       " 'NS': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 5},\n",
       " 'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'OS': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'T': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'W': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " '_': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 5},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchriott\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">soft-terrain-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/chriott/conv-bert-case\" target=\"_blank\">https://wandb.ai/chriott/conv-bert-case</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/chriott/conv-bert-case/runs/1p3ajiy4\" target=\"_blank\">https://wandb.ai/chriott/conv-bert-case/runs/1p3ajiy4</a><br/>\n",
       "                Run data is saved locally in <code>/Users/chris/Downloads/wandb/run-20210322_175356-1p3ajiy4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 33:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.187160</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>6.265900</td>\n",
       "      <td>4.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.568336</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.257732</td>\n",
       "      <td>0.342075</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>5.980700</td>\n",
       "      <td>4.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.084666</td>\n",
       "      <td>0.513126</td>\n",
       "      <td>0.369416</td>\n",
       "      <td>0.429570</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>6.029600</td>\n",
       "      <td>4.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.630459</td>\n",
       "      <td>0.574338</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.525629</td>\n",
       "      <td>0.677333</td>\n",
       "      <td>6.003900</td>\n",
       "      <td>4.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.312608</td>\n",
       "      <td>0.630350</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.591241</td>\n",
       "      <td>0.721333</td>\n",
       "      <td>5.991700</td>\n",
       "      <td>4.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.102588</td>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.654639</td>\n",
       "      <td>0.679750</td>\n",
       "      <td>0.782667</td>\n",
       "      <td>6.014100</td>\n",
       "      <td>4.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.956909</td>\n",
       "      <td>0.751361</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.730803</td>\n",
       "      <td>0.821333</td>\n",
       "      <td>5.992600</td>\n",
       "      <td>4.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.863783</td>\n",
       "      <td>0.811052</td>\n",
       "      <td>0.781787</td>\n",
       "      <td>0.796150</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>6.241300</td>\n",
       "      <td>4.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.812415</td>\n",
       "      <td>0.820285</td>\n",
       "      <td>0.792096</td>\n",
       "      <td>0.805944</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>6.006400</td>\n",
       "      <td>4.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.795551</td>\n",
       "      <td>0.825623</td>\n",
       "      <td>0.797251</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>6.011000</td>\n",
       "      <td>4.492000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: UH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: , seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBZ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NNP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: TO seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRP$ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WDT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EX seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: . seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PDT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RBR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VBD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: POS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WRB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: RBS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: JJR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: HYPH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/chris/opt/anaconda3/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.784265594482422, metrics={'train_runtime': 2009.2121, 'train_samples_per_second': 0.1, 'total_flos': 91308205761360, 'epoch': 10.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inside my model training code\n",
    "import wandb\n",
    "wandb.init(project=\"conv-bert-case\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 100 < 101; dropping {'eval/loss': 0.7623077034950256, 'eval/precision': 0.7707581227436823, 'eval/recall': 0.7336769759450171, 'eval/f1': 0.7517605633802816, 'eval/accuracy': 0.8348387096774194, 'eval/runtime': 8.9998, 'eval/samples_per_second': 3.0, 'train/epoch': 5.0}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7623077034950256,\n",
       " 'eval_precision': 0.7707581227436823,\n",
       " 'eval_recall': 0.7336769759450171,\n",
       " 'eval_f1': 0.7517605633802816,\n",
       " 'eval_accuracy': 0.8348387096774194,\n",
       " 'eval_runtime': 8.9998,\n",
       " 'eval_samples_per_second': 3.0,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': {'precision': 0.8,\n",
       "  'recall': 0.9166666666666666,\n",
       "  'f1': 0.854368932038835,\n",
       "  'number': 48},\n",
       " 'BD': {'precision': 0.5714285714285714,\n",
       "  'recall': 0.8888888888888888,\n",
       "  'f1': 0.6956521739130435,\n",
       "  'number': 9},\n",
       " 'BG': {'precision': 0.8695652173913043,\n",
       "  'recall': 0.9523809523809523,\n",
       "  'f1': 0.909090909090909,\n",
       "  'number': 21},\n",
       " 'BN': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 7},\n",
       " 'BP': {'precision': 1.0,\n",
       "  'recall': 0.7142857142857143,\n",
       "  'f1': 0.8333333333333333,\n",
       "  'number': 14},\n",
       " 'BR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'BS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'BZ': {'precision': 0.8518518518518519,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.92,\n",
       "  'number': 23},\n",
       " 'C': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 18},\n",
       " 'D': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 17},\n",
       " 'DT': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 9},\n",
       " 'H': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4},\n",
       " 'J': {'precision': 0.8620689655172413,\n",
       "  'recall': 0.6944444444444444,\n",
       "  'f1': 0.7692307692307692,\n",
       "  'number': 36},\n",
       " 'JR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'N': {'precision': 0.9104477611940298,\n",
       "  'recall': 0.9312977099236641,\n",
       "  'f1': 0.9207547169811321,\n",
       "  'number': 131},\n",
       " 'NP': {'precision': 0.8545454545454545,\n",
       "  'recall': 0.9591836734693877,\n",
       "  'f1': 0.9038461538461537,\n",
       "  'number': 49},\n",
       " 'NS': {'precision': 0.90625,\n",
       "  'recall': 0.9666666666666667,\n",
       "  'f1': 0.9354838709677419,\n",
       "  'number': 30},\n",
       " 'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 8},\n",
       " 'OS': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3},\n",
       " 'P': {'precision': 1.0,\n",
       "  'recall': 0.4,\n",
       "  'f1': 0.5714285714285715,\n",
       "  'number': 15},\n",
       " 'RB': {'precision': 0.5,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 1},\n",
       " 'RP': {'precision': 0.8333333333333334,\n",
       "  'recall': 0.9615384615384616,\n",
       "  'f1': 0.8928571428571429,\n",
       "  'number': 26},\n",
       " 'RP$': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 4},\n",
       " 'T': {'precision': 0.8918918918918919,\n",
       "  'recall': 0.9705882352941176,\n",
       "  'f1': 0.9295774647887325,\n",
       "  'number': 68},\n",
       " 'X': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4},\n",
       " 'YPH': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " '_': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 32},\n",
       " 'overall_precision': 0.8869565217391304,\n",
       " 'overall_recall': 0.8762886597938144,\n",
       " 'overall_f1': 0.881590319792567,\n",
       " 'overall_accuracy': 0.9225806451612903}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2748<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.04MB of 0.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/chris/Downloads/wandb/run-20210322_175356-1p3ajiy4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/chris/Downloads/wandb/run-20210322_175356-1p3ajiy4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>0.79555</td></tr><tr><td>eval/precision</td><td>0.82562</td></tr><tr><td>eval/recall</td><td>0.79725</td></tr><tr><td>eval/f1</td><td>0.81119</td></tr><tr><td>eval/accuracy</td><td>0.872</td></tr><tr><td>eval/runtime</td><td>6.011</td></tr><tr><td>eval/samples_per_second</td><td>4.492</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>_runtime</td><td>2011</td></tr><tr><td>_timestamp</td><td>1616434047</td></tr><tr><td>_step</td><td>200</td></tr><tr><td>train/train_runtime</td><td>2009.2121</td></tr><tr><td>train/train_samples_per_second</td><td>0.1</td></tr><tr><td>train/total_flos</td><td>91308205761360</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>eval/loss</td><td>█▆▅▃▃▂▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▄▅▅▆▇▇███</td></tr><tr><td>eval/recall</td><td>▁▂▄▅▆▇▇███</td></tr><tr><td>eval/f1</td><td>▁▃▄▅▆▇▇███</td></tr><tr><td>eval/accuracy</td><td>▁▃▅▆▆▇▇███</td></tr><tr><td>eval/runtime</td><td>█▁▂▂▁▂▁▇▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁█▇▇█▇█▂▇▇</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>_runtime</td><td>▁▂▃▄▅▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▃▄▅▅▆▇██</td></tr><tr><td>_step</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">soft-terrain-1</strong>: <a href=\"https://wandb.ai/chriott/conv-bert-case/runs/1p3ajiy4\" target=\"_blank\">https://wandb.ai/chriott/conv-bert-case/runs/1p3ajiy4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#fig = plt.figure()\n",
    "#plt.plot(train_counter, train_losses, color='blue')\n",
    "#plt.scatter(test_counter, test_losses, color='red')\n",
    "#plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "#plt.xlabel('number of training examples seen')\n",
    "#plt.ylabel('negative log likelihood loss')\n",
    "#fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
